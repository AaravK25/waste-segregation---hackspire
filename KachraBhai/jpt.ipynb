{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e6b36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print('System Version:', sys.version)\n",
    "print('PyTorch version', torch.__version__)\n",
    "print('Torchvision version', torchvision.__version__)\n",
    "print('Numpy version', np.__version__)\n",
    "print('Pandas version', pd.__version__)\n",
    "\n",
    "\n",
    "\n",
    "class wastedata(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = ImageFolder(data_dir, transform=transform)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.data.classes\n",
    "    \n",
    "\n",
    "dataset = wastedata(\n",
    "    data_dir=\"C:/Users/aarav/OneDrive/Desktop/文档/Hackspire/KachraBhai/train/TrashNet Dataset/trashnet-master/data/dataset-resized/dataset-resized\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "len(dataset)\n",
    "\n",
    "\n",
    "\n",
    "image, label = dataset[3]\n",
    "print(label)\n",
    "image\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \"C:/Users/aarav/OneDrive/Desktop/文档/Hackspire/KachraBhai/train/TrashNet Dataset/trashnet-master/data/dataset-resized/dataset-resized\"\n",
    "target_to_class = {v: k for k, v in ImageFolder(data_dir).class_to_idx.items()}\n",
    "print(target_to_class)\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data_dir = \"C:/Users/aarav/OneDrive/Desktop/文档/Hackspire/KachraBhai/train/TrashNet Dataset/trashnet-master/data/dataset-resized/dataset-resized\"\n",
    "dataset = wastedata(data_dir, transform)\n",
    "\n",
    "\n",
    "\n",
    "image, label = dataset[1]\n",
    "image.shape\n",
    "\n",
    "\n",
    "\n",
    "# iterate over dataset\n",
    "for image, label in dataset:\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "for images, labels in dataloader:\n",
    "    break\n",
    "print(images.shape, labels.shape)\n",
    "\n",
    "\n",
    "print(labels)\n",
    "\n",
    "\n",
    "\n",
    "class wasteClassifer(nn.Module):\n",
    "    def __init__(self, num_classes=53):\n",
    "        super(wasteClassifer, self).__init__()\n",
    "        # Where we define all the parts of the model\n",
    "        self.base_model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "\n",
    "        enet_out_size = 1280\n",
    "        # Make a classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(enet_out_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Connect these parts and return the output\n",
    "        x = self.features(x)\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "model = wasteClassifer(num_classes=3)\n",
    "print(str(model)[:500])\n",
    "\n",
    "\n",
    "\n",
    "example_out = model(images)\n",
    "example_out.shape # [batch_size, num_classes]\n",
    "\n",
    "\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "criterion(example_out, labels)\n",
    "print(example_out.shape, labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_folder = \"C:/Users/aarav/OneDrive/Desktop/文档/Hackspire/KachraBhai/train\"\n",
    "valid_folder = \"C:/Users/aarav/OneDrive/Desktop/文档/Hackspire/KachraBhai/valid\"\n",
    "test_folder = \"C:/Users/aarav/OneDrive/Desktop/文档/Hackspire/KachraBhai/test\"\n",
    "\n",
    "train_dataset = wastedata(train_folder, transform=transform)\n",
    "val_dataset = wastedata(valid_folder, transform=transform)\n",
    "test_dataset = wastedata(test_folder, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Simple training loop\n",
    "num_epochs = 5\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = wasteClassifer(num_classes=53)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc='Training loop'):\n",
    "        # Move inputs and labels to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc='Validation loop'):\n",
    "            # Move inputs and labels to the device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "         \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Validation loss: {val_loss}\")\n",
    "    \n",
    "\n",
    "torch.save(model.state_dict(),r\"C:/Users/aarav/OneDrive/Desktop/文档/Hackspire/KachraBhai/tp.pt\" )\n",
    "\n",
    "    \n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "model = wasteClassifer(num_classes=53)\n",
    "model.load_state_dict(torch.load(r\"C:/Users/aarav/OneDrive/Desktop/文档/Hackspire/KachraBhai/tp.pt\", weights_only=True))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390633a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
